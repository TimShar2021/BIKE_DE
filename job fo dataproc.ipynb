{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91aa8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2753826",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = '/home/timur/ZOOMDE_PROJECT/keys/project-24508-356bfc7cf395.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2e5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 01:52:27 WARN Utils: Your hostname, timur-IdeaPad-5-14ALC05 resolves to a loopback address: 127.0.1.1; using 192.168.88.253 instead (on interface wlo1)\n",
      "23/04/27 01:52:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/04/27 01:52:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd21939",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc052ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark \\\n",
    "    .read \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .parquet(\"gs://dtc_data_lake_project-24508/data/year=2015/month=1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a998cfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rental_id',\n",
       " 'duration',\n",
       " 'bike_id',\n",
       " 'end_date',\n",
       " 'end_station_id',\n",
       " 'end_station_name',\n",
       " 'start_date',\n",
       " 'start_station_id',\n",
       " 'start_station_name']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87f3f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|date_trunc(day, start_date)|\n",
      "+---------------------------+\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "|        2015-01-17 00:00:00|\n",
      "+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.select(date_trunc(\"day\", \"start_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02fe7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.select('rental_id',\n",
    " 'duration',\n",
    " 'bike_id',\n",
    " date_trunc(\"day\", \"end_date\").alias(\"end_date\"),\n",
    " 'end_station_id',\n",
    " 'end_station_name',\n",
    " date_trunc(\"day\", \"start_date\").alias(\"start_date\"),\n",
    " 'start_station_id',\n",
    " 'start_station_name')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1327bcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-------------------+--------------+--------------------+-------------------+----------------+--------------------+\n",
      "|rental_id|duration|bike_id|           end_date|end_station_id|    end_station_name|         start_date|start_station_id|  start_station_name|\n",
      "+---------+--------+-------+-------------------+--------------+--------------------+-------------------+----------------+--------------------+\n",
      "| 40628023|     600|   4226|2015-01-18 00:00:00|           485|Old Ford Road, Be...|2015-01-17 00:00:00|             692|Cadogan Close, Vi...|\n",
      "| 40627889|    1740|  12539|2015-01-18 00:00:00|           293|Kensington Olympi...|2015-01-17 00:00:00|             109|  Soho Square , Soho|\n",
      "| 40627985|     900|  11362|2015-01-18 00:00:00|           212|Campden Hill Road...|2015-01-17 00:00:00|             296|Knaresborough Pla...|\n",
      "| 40627971|    1020|   4660|2015-01-18 00:00:00|           131|Eversholt Street ...|2015-01-17 00:00:00|             713|Hawley Crescent, ...|\n",
      "| 40627962|    1080|   9912|2015-01-18 00:00:00|           131|Eversholt Street ...|2015-01-17 00:00:00|             713|Hawley Crescent, ...|\n",
      "| 40627067|    9600|  12880|2015-01-18 00:00:00|           252|Jubilee Gardens, ...|2015-01-17 00:00:00|             252|Jubilee Gardens, ...|\n",
      "| 40628034|     480|  12645|2015-01-18 00:00:00|           356|South Kensington ...|2015-01-17 00:00:00|             428|Exhibition Road, ...|\n",
      "| 40628008|     660|  11711|2015-01-18 00:00:00|           588|Hoxton Street, Ho...|2015-01-17 00:00:00|             135|Clerkenwell Green...|\n",
      "| 40628044|     420|   7481|2015-01-18 00:00:00|           155|Lexham Gardens, K...|2015-01-17 00:00:00|             392|Imperial College,...|\n",
      "| 40628054|     240|   7275|2015-01-18 00:00:00|           356|South Kensington ...|2015-01-17 00:00:00|             428|Exhibition Road, ...|\n",
      "| 40627819|    2400|   8157|2015-01-18 00:00:00|           340|Bank of England M...|2015-01-17 00:00:00|             481|Saunders Ness Roa...|\n",
      "| 40627829|    2280|   9747|2015-01-18 00:00:00|           340|Bank of England M...|2015-01-17 00:00:00|             481|Saunders Ness Roa...|\n",
      "| 40627937|    1440|   3293|2015-01-18 00:00:00|           401|Columbia Road, We...|2015-01-17 00:00:00|              39|Shoreditch High S...|\n",
      "| 40627989|     960|   7877|2015-01-18 00:00:00|           298|Curlew Street, Sh...|2015-01-17 00:00:00|              39|Shoreditch High S...|\n",
      "| 40628058|     300|   8260|2015-01-18 00:00:00|           151|Chepstow Villas, ...|2015-01-17 00:00:00|             643|All Saints' Road,...|\n",
      "| 40627988|    1020|   9079|2015-01-18 00:00:00|           486|Granby Street, Sh...|2015-01-17 00:00:00|             298|Curlew Street, Sh...|\n",
      "| 40628037|     600|  12532|2015-01-18 00:00:00|           282|Royal London Hosp...|2015-01-17 00:00:00|              55|Finsbury Circus, ...|\n",
      "| 40627785|    2760|   1509|2015-01-18 00:00:00|           506|Bell Lane, Liverp...|2015-01-17 00:00:00|             296|Knaresborough Pla...|\n",
      "| 40628009|     780|   4041|2015-01-18 00:00:00|           700|Battersea Church ...|2015-01-17 00:00:00|             432|Exhibition Road M...|\n",
      "| 40628050|     420|  12414|2015-01-18 00:00:00|           507|Clarkson Street, ...|2015-01-17 00:00:00|              39|Shoreditch High S...|\n",
      "+---------+--------+-------+-------------------+--------------+--------------------+-------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "555d7b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa41750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.sql(\"\"\"\n",
    "            SELECT end_date as date, \n",
    "            count(bike_id) as count_roads,\n",
    "            ROUND(AVG(duration/60),1) as avg_duration_minutes\n",
    "            FROM data\n",
    "            Group BY  end_date \n",
    "            ORDER BY date ASC;\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4234cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.sql(\"\"\"SELECT S.date,S.station_name,S.start_count,E.end_count\n",
    "                                FROM\n",
    "                                    (SELECT  start_date as date, \n",
    "                                    count(bike_id) as start_count,\n",
    "                                    start_station_name as station_name\n",
    "                                    FROM data\n",
    "                                    Group BY   date,start_station_name\n",
    "                                    ORDER BY date ASC) AS S\n",
    "                                FULL OUTER JOIN\n",
    "                                    (SELECT start_date as date, \n",
    "                                    count(bike_id) as end_count,\n",
    "                                    end_station_name \n",
    "                                    FROM data\n",
    "                                    Group BY   date,end_station_name\n",
    "                                    ORDER BY date ASC) AS E\n",
    "                                ON S.date = E.date AND S.station_name = E.end_station_name;\n",
    "                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9e056da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+---------+\n",
      "|               date|        station_name|start_count|end_count|\n",
      "+-------------------+--------------------+-----------+---------+\n",
      "|2015-01-04 00:00:00|Abingdon Villas, ...|         20|       26|\n",
      "|2015-01-04 00:00:00|  Ackroyd Drive, Bow|          6|        4|\n",
      "|2015-01-04 00:00:00|Albert Bridge Roa...|         40|       52|\n",
      "|2015-01-04 00:00:00|Albert Gate, Hyde...|         78|       86|\n",
      "|2015-01-04 00:00:00|Alderney Street, ...|         38|       36|\n",
      "|2015-01-04 00:00:00|Alfred Place, Blo...|         18|       18|\n",
      "|2015-01-04 00:00:00|All Saints Church...|         40|       38|\n",
      "|2015-01-04 00:00:00|All Saints' Road,...|         16|       16|\n",
      "|2015-01-04 00:00:00|Alma Road, Wandsw...|         12|       10|\n",
      "|2015-01-04 00:00:00|Altab Ali Park, W...|         58|       40|\n",
      "|2015-01-04 00:00:00|Antill Road, Mile...|         12|       14|\n",
      "|2015-01-04 00:00:00|Appold Street, Li...|         16|        8|\n",
      "|2015-01-04 00:00:00|Ashley Place, Vic...|         24|       38|\n",
      "|2015-01-04 00:00:00|Bancroft Road, Be...|         26|       18|\n",
      "|2015-01-04 00:00:00|Bank of England M...|          6|       10|\n",
      "|2015-01-04 00:00:00|Barbican Centre, ...|         16|       32|\n",
      "|2015-01-04 00:00:00|Barons Court Stat...|         18|       20|\n",
      "|2015-01-04 00:00:00|Battersea Church ...|         26|       22|\n",
      "|2015-01-04 00:00:00|Bayswater Road, H...|         64|       66|\n",
      "|2015-01-04 00:00:00|Bedford Way, Bloo...|         24|       16|\n",
      "+-------------------+--------------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42ab5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1 = spark.sql(\"\"\"SELECT start_date\n",
    "       FROM data;\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bef15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         start_date|\n",
      "+-------------------+\n",
      "|2015-01-17 23:50:00|\n",
      "|2015-01-17 23:31:00|\n",
      "|2015-01-17 23:45:00|\n",
      "|2015-01-17 23:43:00|\n",
      "|2015-01-17 23:42:00|\n",
      "|2015-01-17 21:20:00|\n",
      "|2015-01-17 23:52:00|\n",
      "|2015-01-17 23:49:00|\n",
      "|2015-01-17 23:53:00|\n",
      "|2015-01-17 23:56:00|\n",
      "|2015-01-17 23:21:00|\n",
      "|2015-01-17 23:23:00|\n",
      "|2015-01-17 23:37:00|\n",
      "|2015-01-17 23:46:00|\n",
      "|2015-01-17 23:57:00|\n",
      "|2015-01-17 23:45:00|\n",
      "|2015-01-17 23:52:00|\n",
      "|2015-01-17 23:16:00|\n",
      "|2015-01-17 23:49:00|\n",
      "|2015-01-17 23:55:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eeaaaaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Partition column `year` not found in schema struct<rental_id:int,duration:int,bike_id:int,end_date:timestamp,end_station_id:int,end_station_name:string,start_date:timestamp,start_station_id:int,start_station_name:string>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_149636/3926303727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdft\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Partition column `year` not found in schema struct<rental_id:int,duration:int,bike_id:int,end_date:timestamp,end_station_id:int,end_station_name:string,start_date:timestamp,start_station_id:int,start_station_name:string>"
     ]
    }
   ],
   "source": [
    "dft\\\n",
    "    .write \\\n",
    "    .format(\"parquet\")\\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"gs://dtc_data_lake_de-71680/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ba834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9597b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 23:57:25 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://dtc_data_lake_de-71680/data/year=2016/mount=1/.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o107.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_147821/2166769483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://dtc_data_lake_de-71680/data/year=2016/mount=1/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    362\u001b[0m         )\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     def text(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o107.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "data = spark \\\n",
    "    .read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet(\"gs://dtc_data_lake_de-71680/data/year=2016/mount=1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314d2995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-------------------+-------------+--------------------+-------------------+---------------+--------------------+\n",
      "|Rental Id|Duration|Bike Id|           End Date|EndStation Id|     EndStation Name|         Start Date|StartStation Id|   StartStation Name|\n",
      "+---------+--------+-------+-------------------+-------------+--------------------+-------------------+---------------+--------------------+\n",
      "| 50754225|     240|  11834|2016-01-10 00:04:00|          383|  Frith Street, Soho|2016-01-10 00:00:00|             18|Drury Lane, Coven...|\n",
      "| 50754226|     300|   9648|2016-01-10 00:05:00|          719|Victoria Park Roa...|2016-01-10 00:00:00|            479|Pott Street, Beth...|\n",
      "| 50754227|    1200|  10689|2016-01-10 00:20:00|          272|Baylis Road, Wate...|2016-01-10 00:00:00|            425|Harrington Square...|\n",
      "| 50754228|     780|   8593|2016-01-10 00:14:00|          471|Hewison Street, O...|2016-01-10 00:01:00|            487|Canton Street, Po...|\n",
      "| 50754229|     600|   8619|2016-01-10 00:11:00|          399|Brick Lane Market...|2016-01-10 00:01:00|            501|Cephas Street, Be...|\n",
      "| 50754230|     420|    309|2016-01-10 00:09:00|          671|Parsons Green Sta...|2016-01-10 00:02:00|            769|Sandilands Road, ...|\n",
      "| 50754231|     960|  11914|2016-01-10 00:18:00|          450|Jubilee Street, S...|2016-01-10 00:02:00|            516|Chrisp Street Mar...|\n",
      "| 50754232|     480|   3314|2016-01-10 00:11:00|          780|Imperial Wharf St...|2016-01-10 00:03:00|            755|The Vale, West Ch...|\n",
      "| 50754233|     240|   2825|2016-01-10 00:08:00|          638|Falcon Road, Clap...|2016-01-10 00:04:00|            701|Vicarage Crescent...|\n",
      "| 50754234|    1320|   3102|2016-01-10 00:27:00|          647|Richmond Way, She...|2016-01-10 00:05:00|            633|Vereker Road Nort...|\n",
      "| 50754235|     480|   4261|2016-01-10 00:13:00|           86|Sancroft Street, ...|2016-01-10 00:05:00|            420|Southwark Station...|\n",
      "| 50754236|    1020|   8051|2016-01-10 00:22:00|          600|South Lambeth Roa...|2016-01-10 00:05:00|            386|   Moor Street, Soho|\n",
      "| 50754237|     780|   4184|2016-01-10 00:19:00|          552|Watney Street, Sh...|2016-01-10 00:06:00|            534|Goldsmith's Row, ...|\n",
      "| 50754238|     720|   2855|2016-01-10 00:18:00|          518|Antill Road, Mile...|2016-01-10 00:06:00|            717|Dunston Road , Ha...|\n",
      "| 50754239|     420|  12698|2016-01-10 00:14:00|          522|Clinton Road, Mil...|2016-01-10 00:07:00|            521|Driffield Road, O...|\n",
      "| 50754241|     180|  12003|2016-01-10 00:11:00|           78|Sadlers Sports Ce...|2016-01-10 00:08:00|            264|Tysoe Street, Cle...|\n",
      "| 50754242|    1140|  11173|2016-01-10 00:27:00|          409|Strata, Elephant ...|2016-01-10 00:08:00|            486|Granby Street, Sh...|\n",
      "| 50754243|     480|    779|2016-01-10 00:16:00|           22|Northington Stree...|2016-01-10 00:08:00|            331|Bunhill Row, Moor...|\n",
      "| 50754244|     420|   2406|2016-01-10 00:15:00|          396|Shouldham Street,...|2016-01-10 00:08:00|            397|Devonshire Terrac...|\n",
      "| 50754245|     540|    436|2016-01-10 00:17:00|          242|Beaumont Street, ...|2016-01-10 00:08:00|              7|Charlbert Street,...|\n",
      "+---------+--------+-------+-------------------+-------------+--------------------+-------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908be169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_1 = data.select(\n",
    "    [\"EndStation Id\", \"EndStation Name\"])\n",
    "\n",
    "df_2 = data.select(\n",
    "    [\"StartStation Id\", \"StartStation Name\"])\n",
    "\n",
    "df_2 = df_2.withColumnRenamed(\"StartStation Id\", \"Station_Id\")\\\n",
    "       .withColumnRenamed(\"StartStation Name\", \"Station_Name\")\n",
    "\n",
    "df_1 = df_1.withColumnRenamed(\"EndStation Id\", \"Station_Id\")\\\n",
    "       .withColumnRenamed(\"EndStation Name\", \"Station_Name\")\n",
    "\n",
    "\n",
    "station = df_1.union(df_2)\n",
    "\n",
    "station.registerTempTable('station')\n",
    "\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    Station_Id, \n",
    "    Station_Name\n",
    "FROM\n",
    "    station\n",
    "GROUP BY \n",
    "  1,2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbee7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Station_Id|        Station_Name|\n",
      "+----------+--------------------+\n",
      "|       520|Bancroft Road, Be...|\n",
      "|       352|Vauxhall Street, ...|\n",
      "|       368|Harriet Street, K...|\n",
      "|       467| Southern Grove, Bow|\n",
      "|       740|Sirdar Road, Avon...|\n",
      "|       402|Penfold Street, M...|\n",
      "|       750|Culvert Road, Bat...|\n",
      "|       686|Beryl Road, Hamme...|\n",
      "|       433|Wren Street, Holborn|\n",
      "|       764|St. John's Road, ...|\n",
      "|       105|Westbourne Grove,...|\n",
      "|       748|Hertford Road, De...|\n",
      "|       677|Heath Road, Batte...|\n",
      "|       729|St. Peter's Terra...|\n",
      "|       312|Grove End Road, S...|\n",
      "|       456|Parkway, Camden Town|\n",
      "|       144|Kennington Cross,...|\n",
      "|        57|Guilford Street ,...|\n",
      "|       247|St. John's Wood C...|\n",
      "|       448|Fisherman's Walk ...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4171ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7559/2476272088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"writeMethod\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"direct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"writeMethod\", \"direct\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f709b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"dataproc-temp-us-central1-857387732128-knznbrvo\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bc7ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Station Id|        Station Name|\n",
      "+----------+--------------------+\n",
      "|       383|  Frith Street, Soho|\n",
      "|       719|Victoria Park Roa...|\n",
      "|       272|Baylis Road, Wate...|\n",
      "|       471|Hewison Street, O...|\n",
      "|       399|Brick Lane Market...|\n",
      "|       671|Parsons Green Sta...|\n",
      "|       450|Jubilee Street, S...|\n",
      "|       780|Imperial Wharf St...|\n",
      "|       638|Falcon Road, Clap...|\n",
      "|       647|Richmond Way, She...|\n",
      "|        86|Sancroft Street, ...|\n",
      "|       600|South Lambeth Roa...|\n",
      "|       552|Watney Street, Sh...|\n",
      "|       518|Antill Road, Mile...|\n",
      "|       522|Clinton Road, Mil...|\n",
      "|        78|Sadlers Sports Ce...|\n",
      "|       409|Strata, Elephant ...|\n",
      "|        22|Northington Stree...|\n",
      "|       396|Shouldham Street,...|\n",
      "|       242|Beaumont Street, ...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08944fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dd54327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"SELECT *\n",
    "FROM trips_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a256f7db",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o142.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:587)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:675)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:864)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:661)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:661)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10322/3150535694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporaryGcsBucket\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtc_data_lake_de-71680\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"database\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"de-71680.bicycles_data_all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o142.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:587)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:675)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:864)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:661)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:661)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "dft.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"temporaryGcsBucket\", \"dtc_data_lake_de-71680\")\\\n",
    "    .option(\"database\", \"de-71680.bicycles_data_all\")\\\n",
    "    .option(\"table\", \"de-71680.bicycles_data_all.station\")\\\n",
    "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c0ec06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Station Id|        Station Name|\n",
      "+----------+--------------------+\n",
      "|       520|Bancroft Road, Be...|\n",
      "|       352|Vauxhall Street, ...|\n",
      "|       368|Harriet Street, K...|\n",
      "|       467| Southern Grove, Bow|\n",
      "|       740|Sirdar Road, Avon...|\n",
      "|       402|Penfold Street, M...|\n",
      "|       750|Culvert Road, Bat...|\n",
      "|       686|Beryl Road, Hamme...|\n",
      "|       433|Wren Street, Holborn|\n",
      "|       764|St. John's Road, ...|\n",
      "|       105|Westbourne Grove,...|\n",
      "|       748|Hertford Road, De...|\n",
      "|       677|Heath Road, Batte...|\n",
      "|       729|St. Peter's Terra...|\n",
      "|       312|Grove End Road, S...|\n",
      "|       456|Parkway, Camden Town|\n",
      "|       144|Kennington Cross,...|\n",
      "|        57|Guilford Street ,...|\n",
      "|       247|St. John's Wood C...|\n",
      "|       448|Fisherman's Walk ...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757f9f87",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (2923426471.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_10322/2923426471.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df_censo.write\\\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "df_censo.write\\ \n",
    "    . формат ( «большой запрос» )\\ \n",
    "    .mode ( «перезаписать» )\\ \n",
    "    .option ( «temporaryGcsBucket» , «censo-ensino-superior» )\\ \n",
    "    .option ( «база данных» , «censo_ensino_superior» )\\ \n",
    "    .option ( «таблица» , \"censo_ensino_superior.cursos_graduacao_e_licenciatura\" )\\ \n",
    "    .option( \"createDisposition\" , \"CREATE_IF_NEEDED\" )\\ \n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
